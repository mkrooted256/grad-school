
\section{Класифікація текстур}\label{section1.3}
% Майборода. Непараметрична статистика

% один дескриптор
Нехай спостерігаються зображення $I \colon K \to \{\overline{0,255}\}$ 
\footnote{$K$ -- множина координат зображення, одна для всіх зображень. Вважаємо, що всі зображення одного розміру.}
, кожне з яких містить одну з $S$ різних текстур, які позначатимемо номерами $\overline{1,S}$.
Текстуру, що відповідає зображенню, позначатимемо $\kappa(I)$.
Використовуємо один текстурний дескриптор $T$ із областю значень $D = \{\overline{T_{min}, T_{max}}\} \subset \Z$ і обчислюємо його значення в кожному пікселі. 
Зображенню відповідає набір з $\# K$ значень який вважатимемо випадковою величиною
\[ T(I) = \left(T(I,c),\; c\in K\right) \colon \Omega \to D^{\# K}, \]
при цьому $T(I,c)$ -- незалежні за $c$ й однаково розподілені за законом $F$.
Припускаємо, що текстура характеризується \emph{розподілом} текстурного дескриптора $T$, тобто розподіл $F = F_{\kappa(I)}$ залежить від $\kappa(I)$.
Задачею є деяким чином оцінити невідоме $\kappa(I)$ за спостереженими $T(I,c)$.

% % багато дескрипторів
% Нехай спостерігаються зображення $I \colon K \to \{\overline{0,255}\}$, кожне з яких містить одну з $S$ різних текстур, які позначатимемо номерами.
% Текстуру, що відповідає зображенню, позначатимемо $\kappa(I)$.
% Використовуємо $M$ незалежних текстурних дескрипторів $T^m$ і обчислюємо їх значення в кожному пікселі. 
% Для зручності, припускаємо, що всі $T$ мають одну область значень $D = \{\overline{T^m_{min}, T^m_{max}}\} \subset \Z$.
% Кожному зображенню відповідає набір з $\# K$ значень кожного з $M$ дескрипторів, який вважатимемо випадковою величиною
% \[ T^m(I) = \left(T^m(I,c),\; c\in K\right) \colon \Omega \to D^{\# K}, \]
% при цьому $T^m(I,c)$ -- незалежні за $m$ та $c$, та однаково розподілені за законом $F_m$ ($T^m$ незалежні та по-різному розподілені).
% Припускаємо, що текстура характеризується \emph{розподілами} текстурних дескрипторів $T^m$, тобто розподіли $F_m$ залежать від $\kappa(I)$.
% Задачею є деяким чином оцінити невідоме $\kappa(I)$ за спостереженими $T^m(I,c)$.

Припустимо, що ми також маємо навчальну вибірку, тобто для певної кількості інших зображень $I^t$ відомі дійсні значення $\kappa(I^t)$. 
Для деякого статистичного критерію близкості розподілів із статистикою $X$ та критичною областю $X_c$ можна побудувати серію критеріїв $X^{(s)}$ 
для оцінки близкості до кожної відомої текстури і на їх основі побудувати класифікатор.

\subsection{Критерій $\chi^2$}\label{section1.3a}
% https://stats.stackexchange.com/questions/1047/is-kolmogorov-smirnov-test-valid-with-discrete-distributions
% todo: normality test?????

Припустимо, що використовуємо один текстурний дескриптор $T$.
В кожному пікселі $\P(T = d) = p^{(s)}_d$, і при цьому значення $T$ не залежить від значень у інших пікселях.
Тоді, розподіл частот $\left(\nu_d, d\in D\right)$ є мультиноміальним із імовірностями $p^{(s)}_d$ та 
кількістю спостережень рівною кількістю пікселів на зображенні $\# K$, $\sum_{d}\nu_d = \# K$. 
Закон розподілу має наступний вигляд
\begin{equation*}
    f_s(\{ \nu_d \}) = \frac {\Gamma (\sum_{d}\nu_{d}+1)}{\prod_{d}\Gamma (\nu_{d}+1)} \prod (p^{(s)}_d)^{\nu_d}
\end{equation*}

Нехай $e^{(s)}_d, d \in D$ -- усереднена частота появи значення $d$ дескриптора $T$ на зображеннях, яким відповідає відома текстура $s$
($e^{(s)}_d = \# K \cdot p^{(s)}_d$), а $\nu_d$ -- частота появи значення на зображенні, текстуру якого ми класифікуємо.

Нульовою гіпотезою вважаємо те, що розподіли $T$ для текстури $s$ та на оцінюваному зображенні рівні.
Якщо припустити, що за нульової гіпотези спостережувані частоти є нормально розподіленими навколо очікуваного значення, $\nu_d \sim \mathcal N(e^{(s)}_d, )$, то можна розглядати статистику
\begin{equation*}\label{e:chi2crit}
    X(\vec \nu,\vec e^{(s)}) = \sum_{d \in D} \frac{\left(\nu_d - e^{(s)}_d\right)^2}{e^{(s)}_d} \longrightarrow \chi^2_{\# D - 1}
\end{equation*}
Для рівня значущості $\alpha$ критичним значенням буде, відповідно, квантиль рівня $1-\alpha$ розподілу $\chi^2$ із $\# D - 1$ степенями свободи.
Зауважу, що на практиці гіпотеза нормальності спостережуваних частот може не підтверджуватись.

Примітивний класифікатор може мати вигляд
\begin{equation*}\label{e:chi2classifier}
    g(I) = \arg \min_{s} X^{(s)},
\end{equation*} 
що мінімізує ймовірність помилки, враховуючи односторонність критерію, та еквівалентно пошуку найближчого $\vec p^{(s)}$ до $\vec \nu$ у $L^2$ сенсі.

\subsection{Статистичний критерій log-правдоподібності}\label{section1.3b}\hfill

Іншим поширеним критерієм близкості розподілів є критерій відношення лог-правдоподібності, або G-test.
Статистика критерію має вигляд 
\begin{equation}\label{e:gtest-1}
    G(\vec \nu,\vec e^{(s)}) = 2\sum_{d \in D} \nu_d \log \frac{\nu_d}{e^{(s)}_d} \longrightarrow \chi^2_{\# D - 1}.
\end{equation}
$G$ також є відстанню Кульбака-Ляйблера між гістограмами $\nu_d$ та $e^{(s)}_d$. 

% потужність порівняно з \chi^2?

Класифікатор тоді може мати вигляд
\begin{equation}\label{e:gtest-classifier-1}
    g(I) = \arg \min_s  \sum_{d \in D} \nu_d \log \frac{\nu_d}{e^{(s)}_d}
\end{equation}
або, еквівалентно,\begin{equation*}\label{e:gtest-classifier-2}
    g(I) = \arg \max_s  \sum_{d \in D} \nu_d \log e^{(s)}_d
\end{equation*}

\subsection{Класифікація за $k$ ближніми сусідами}\label{section1.3c}

Попередні два способи фактично визначають дещо схоже на відстань на просторі зображень і класифікують за близкістю до центру класів.
Точнішим способом є класифікація за $k$ ближніми у деякому сенсі сусідами, де передбачений клас визначається як найчастіший серед сусідів.
Таким чином можна уникнути усереднення гістограм і покращити точність класифікації у випадку, коли текстура може мати різні представлення у вигляді гістограм.
Іншою перевагою є те, що при використанні залежного від обертання дескриптора, для тренування можна використовувати також повернуті варіанти зображень, що дозволить класифікувати зоображення із різними орієнтаціями.



%%
