
\section{Класифікація текстур}\label{section1.3}
% Майборода. Непараметрична статистика

% один дескриптор
Нехай спостерігаються зображення $I \colon K \to \{\overline{0,255}\}$ 
\footnote{$K$ -- множина координат зображення, одна для всіх зображень. Вважаємо, що всі зображення одного розміру.}
, кожне з яких містить одну з $S$ різних текстур, які позначатимемо номерами $\overline{1,S}$.
Текстуру, що відповідає зображенню, позначатимемо $\kappa(I)$.
Використовуємо один текстурний дескриптор $T$ із областю значень $D = \{\overline{T_{min}, T_{max}}\} \subset \Z$ і обчислюємо його значення в кожному пікселі. 
Зображенню відповідає набір з $\# K$ значень який вважатимемо випадковою величиною
\[ T(I) = \left(T(I,c),\; c\in K\right) \colon \Omega \to D^{\# K}, \]
при цьому $T(I,c)$ -- незалежні за $c$ й однаково розподілені за законом $F$.
Припускаємо, що текстура характеризується \emph{розподілом} текстурного дескриптора $T$, тобто розподіл $F = F_{\kappa(I)}$ залежить від $\kappa(I)$.
Задачею є деяким чином оцінити невідоме $\kappa(I)$ за спостереженими $T(I,c)$.

% % багато дескрипторів
% Нехай спостерігаються зображення $I \colon K \to \{\overline{0,255}\}$, кожне з яких містить одну з $S$ різних текстур, які позначатимемо номерами.
% Текстуру, що відповідає зображенню, позначатимемо $\kappa(I)$.
% Використовуємо $M$ незалежних текстурних дескрипторів $T^m$ і обчислюємо їх значення в кожному пікселі. 
% Для зручності, припускаємо, що всі $T$ мають одну область значень $D = \{\overline{T^m_{min}, T^m_{max}}\} \subset \Z$.
% Кожному зображенню відповідає набір з $\# K$ значень кожного з $M$ дескрипторів, який вважатимемо випадковою величиною
% \[ T^m(I) = \left(T^m(I,c),\; c\in K\right) \colon \Omega \to D^{\# K}, \]
% при цьому $T^m(I,c)$ -- незалежні за $m$ та $c$, та однаково розподілені за законом $F_m$ ($T^m$ незалежні та по-різному розподілені).
% Припускаємо, що текстура характеризується \emph{розподілами} текстурних дескрипторів $T^m$, тобто розподіли $F_m$ залежать від $\kappa(I)$.
% Задачею є деяким чином оцінити невідоме $\kappa(I)$ за спостереженими $T^m(I,c)$.

Припустимо, що ми також маємо навчальну вибірку, тобто для певної кількості інших зображень $I^t$ відомі дійсні значення $\kappa(I^t)$. 

\subsection{Критерій $\chi^2$}
% https://stats.stackexchange.com/questions/1047/is-kolmogorov-smirnov-test-valid-with-discrete-distributions
% todo: normality test?????

\subsection{Статистичний критерій log-правдоподібності}\label{section1.3a}\hfill

\todo{G-test and log-likelihood test} \\
% Побудуємо процес класифікації на основі непараметричного критерію $\chi^2$ близкості імоврнісних розподілів статистики $T$ тренувальних та тестових зображень.
% Почнемо з одного зображення. Позначимо гістограму відомого зображення як $M$, невідомого як $S$. 
% Нульовою гіпотезою є те, що два зображення містять різні текстури, що відповідає різним розподілам текстурних дексрипторів $T$.
% Альтернативною гіпотезою є те, що розподіли співпадають і два зображення містять одну і ту ж текстуру.

За нульової гіпотези, \todo{} \\
За альтернативної гіпотези, \todo{} \\
Статистику критерію побудуємо як відношення функцій правдоподібності \todo{}  \\
Спрощуємо статистику критерію до 
\begin{equation}\label{e:classify-1}
    G(S,M) = 2\sum_{b=1}^B S_b \log \frac{S_b}{M_b}
\end{equation}
$G$ також є відстанню Кульбака — Лейблера між емпіричними розподілами $S$ та $M$ (?). \\
\todo{КРитична область}

У випадку багатокласового класифікатора із відомими текстурами $M_i$, спостережуваному зображенню $S$ можна призначити клас текстури $M_i$ такий,
що максимізує величину
\begin{equation}\label{e:classify-2}
    L(S,M) = \sum_{b=1}^B S_b \log M_b.
\end{equation}

\todo{}

Отже, вектором ознак зображення може бути гістограма (як послідовність частот), конкатенація гістограм для різних параметрів, або значення комірок багатовимірної гістограми.
Поширеними класифікаторами є багатокласовий SVM із гаусовим ядром, та K-Nearest-Neighbors із \mbox{(псевдо-)метрикою} близкості розподілів.

%%
